{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4c09c0",
   "metadata": {},
   "source": [
    "# TORCHVISION 객체 검출 미세조정(FINETUNING) 튜토리얼\n",
    "\n",
    "목표: Penn-Fudan Database for Pedestrian Detection and Segmentation 데이터셋으로 미리 학습된 Mask R-CNN 모델을 미세조정하기\n",
    "* 이 데이터셋에는 보행자 인스턴스 345명이 있는 170개의 이미지가 포함됨\n",
    "  * (인스턴스: instance, 역자주: 이미지 내에서 사람의 위치 좌표와 픽셀 단위의 사람 여부를 구분한 정보) \n",
    "* 사용자 정의 데이터셋에 인스턴스 분할(Instance Segmentation) 모델을 학습하기 위해 torchvision의 새로운 기능을 사용하는 방법을 설명\n",
    "\n",
    "![Penn-Fudan Database for Pedestrian Detection](resources/images/penn-fudan-pedestrian-database.png)\n",
    "![Penn-Fudan Database for Pedestrian Segmentation](resources/images/penn-fudan-pedestrian-database-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f45ddd",
   "metadata": {},
   "source": [
    "## 데이터셋 정의하기\n",
    "\n",
    "custom dataset 정의: 객체 검출, 인스턴스 분할 및 사용자 키포인트(Keypoint) 검출을 학습하기 위한 dataset 정의\n",
    "* 데이터셋은 표준 `torch.utils.data.Dataset` 클래스를 상속 받아야 하며, `__len__` 와 `__getitem__` 메소드를 구현해야 함\n",
    "* 우리가 데이터셋 클래스에서 필요로 하는 유일한 특성은 `__getitem__` 를 반환해야 한다는 점\n",
    "\n",
    "Dataset의 속성\n",
    "* 이미지 : PIL(Python Image Library) 이미지의 크기 (H, W)\n",
    "* 대상: 다음의 필드를 포함하는 사전 타입\n",
    "  * `boxes (FloatTensor[N, 4])`: N 개의 바운딩 박스(Bounding box)의 좌표를 [x0, y0, x1, y1] 형태로 가집니다. x와 관련된 값 범위는 0 부터 W 이고 y와 관련된 값의 범위는 0 부터 H 까지입니다.\n",
    "  * `labels (Int64Tensor[N])`: 바운딩 박스 마다의 라벨 정보입니다. 0 은 항상 배경의 클래스를 표현합니다.\n",
    "  * `image_id (Int64Tensor[1])`: 이미지 구분자입니다. 데이터셋의 모든 이미지 간에 고유한 값이어야 하며 평가 중에도 사용됩니다.\n",
    "  * `area (Tensor[N])`: 바운딩 박스의 면적입니다. 면적은 평가 시 작음,중간,큰 박스 간의 점수를 내기 위한 기준이며 COCO 평가를 기준으로 합니다.\n",
    "  * `iscrowd (UInt8Tensor[N])`: 이 값이 참일 경우 평가에서 제외합니다.\n",
    "  * (선택적) `masks (UInt8Tensor[N, H, W])`: N 개의 객체 마다의 분할 마스크 정보입니다.\n",
    "  * (선택적) `keypoints (FloatTensor[N, K, 3])`: N 개의 객체마다의 키포인트 정보\n",
    "      * 키포인트는 [x, y, visibility] 형태의 값입니다. visibility 값이 0인 경우 키포인트는 보이지 않음을 의미합니다. \n",
    "      * 데이터 증강(Data augmentation)의 경우 키포인트 좌우 반전의 개념은 데이터 표현에 따라 달라지며, \n",
    "      새로운 키포인트 표현에 대해 “references/detection/transforms.py” 코드 부분을 수정 해야 할 수도 있습니다.\n",
    "\n",
    "## 데이터 획득하기\n",
    "* 데이터 셋이 위의 정보를 가지고 있으면, 학습과 평가 모두에서 사용이 가능함\n",
    "* 평가 스크립트는 `pip install pycocotools` 로 설치 가능한 `pycocotools` 를 사용\n",
    "* 윈도우즈에서는 `pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI` 명령어를 사용하여 pycocotools 를 gautamchitnis 로부터 가져와 설치\n",
    "  * cython을 설치하지 않았다면 `pip install Cython` 수행 후 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "pip install cython\n",
    "# Install pycocotools, the version by default in Colab\n",
    "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
    "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ada1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "# download the Penn-Fudan dataset\n",
    "wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
    "# extract it in the current folder\n",
    "unzip PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d79679",
   "metadata": {},
   "source": [
    "labels 에 대한 참고사항. \n",
    "* 이 모델은 클래스 0 을 배경으로 취급합니다. 만약 준비한 데이터셋에 배경의 클래스가 없다면, labels 에도 0 이 없어야 합니다. \n",
    "* 예를 들어, 고양이 와 강아지 의 오직 2개의 클래스만 분류한다고 가정하면, (0 이 아닌) 1 이 고양이 를, 2 가 강아지 를 나타내도록 정의해야 합니다. \n",
    "* 따라서, 이 예시에서, 어떤 이미지에 두 개의 클래스를 모두 있다면, labels 텐서는 [1,2] 와 같은 식이 되어야 합니다.\n",
    "\n",
    "* 추가로, 학습 중에 가로 세로 비율 그룹화를 사용하려는 경우(각 배치에 유사한 가로 세로 비율이 있는 영상만 포함되도록), 이미지의 넓이, 높이를 리턴할 수 있도록 get_height_and_width 메소드를 구현하기를 추천합니다. \n",
    "* 이 메소드가 구현되지 않은 경우에는 모든 데이터셋은 __getitem__ 를 통해 메모리에 이미지가 로드되며 사용자 정의 메소드를 제공하는 것보다 느릴 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd3e0d",
   "metadata": {},
   "source": [
    "## PennFudan를 위한 사용자 정의 데이터셋 작성하기\n",
    "PennFudan 데이터셋을 위한 코드를 작성해 보겠습니다. `다운로드 후 압축 파일을 해제하면<https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip>`__, 다음의 폴더 구조를 볼 수 있습니다:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fcb77",
   "metadata": {},
   "source": [
    "labels 에 대한 참고사항. \n",
    "* 이 모델은 클래스 0 을 배경으로 취급합니다. \n",
    "* 만약 준비한 데이터셋에 배경의 클래스가 없다면, labels 에도 0 이 없어야 합니다. \n",
    "* 예를 들어, 고양이 와 강아지 의 오직 2개의 클래스만 분류한다고 가정하면, (0 이 아닌) 1 이 고양이 를, 2 가 강아지 를 나타내도록 정의해야 합니다. \n",
    "* 따라서, 이 예시에서, 어떤 이미지에 두 개의 클래스를 모두 있다면, labels 텐서는 [1,2] 와 같은 식이 되어야 합니다.\n",
    "\n",
    "추가로, 학습 중에 가로 세로 비율 그룹화를 사용하려는 경우(각 배치에 유사한 가로 세로 비율이 있는 영상만 포함되도록), 이미지의 넓이, 높이를 리턴할 수 있도록 `get_height_and_width` 메소드를 구현하기를 추천합니다. \n",
    "* 이 메소드가 구현되지 않은 경우에는 모든 데이터셋은 __getitem__ 를 통해 메모리에 이미지가 로드되며 사용자 정의 메소드를 제공하는 것보다 느릴 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df336f7",
   "metadata": {},
   "source": [
    "PennFudan를 위한 사용자 정의 데이터셋 작성하기\n",
    "PennFudan 데이터셋을 위한 코드를 작성해 보겠습니다. `다운로드 후 압축 파일을 해제하면<https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip>`__, 다음의 폴더 구조를 볼 수 있습니다:\n",
    "```\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    FudanPed00003_mask.png\n",
    "    FudanPed00004_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    FudanPed00003.png\n",
    "    FudanPed00004.png\n",
    "```\n",
    "한 쌍의 영상과 분할 마스크의 한 가지 예는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f78baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # 모든 이미지 파일들을 읽고, 정렬하여\n",
    "        # 이미지와 분할 마스크 정렬을 확인합니다\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지와 마스크를 읽어옵니다\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # 분할 마스크는 RGB로 변환하지 않음을 유의하세요\n",
    "        # 왜냐하면 각 색상은 다른 인스턴스에 해당하며, 0은 배경에 해당합니다\n",
    "        mask = Image.open(mask_path)\n",
    "        # numpy 배열을 PIL 이미지로 변환합니다\n",
    "        mask = np.array(mask)\n",
    "        # 인스턴스들은 다른 색들로 인코딩 되어 있습니다.\n",
    "        obj_ids = np.unique(mask)\n",
    "        # 첫번째 id 는 배경이라 제거합니다\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # 컬러 인코딩된 마스크를 바이너리 마스크 세트로 나눕니다\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # 각 마스크의 바운딩 박스 좌표를 얻습니다\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # 모든 것을 torch.Tensor 타입으로 변환합니다\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # 객체 종류는 한 종류만 존재합니다(역자주: 예제에서는 사람만이 대상입니다)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # 모든 인스턴스는 군중(crowd) 상태가 아님을 가정합니다\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PennFudanDataset('PennFudanPed/')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51efbee",
   "metadata": {},
   "source": [
    "## 모델 정의하기\n",
    "\n",
    "이번 튜토리얼에서는 Faster R-CNN 에 기반한 Mask R-CNN 모델을 사용할 예정입니다. Faster R-CNN은 이미지에 존재할 수 있는 객체에 대한 바운딩 박스와 클래스 점수를 모두 예측하는 모델입니다.\n",
    "\n",
    "![Faster R-CNN](resources/images/faster-rcnn.png)\n",
    "\n",
    "Mask R-CNN은 각 인스턴스에 대한 분할 마스크 예측하는 추가 분기(레이어)를 Faster R-CNN에 추가한 모델입니다.\n",
    "\n",
    "![Mask R-CNN](resources/images/mask-rcnn.png)\n",
    "\n",
    "### Torchvision 모델 주(Model Zoo)에서 모델을 가져와 수정 하는 방법\n",
    "1. 첫 번째 방법은 미리 학습된 모델에서 시작해서 마지막 레이어 수준만 미세 조정하는 것입니다. \n",
    "2. 다른 하나는 모델의 백본을 다른 백본으로 교체하는 것입니다.(예를 들면, 더 빠른 예측을 하려고 할때) (역자주: 백본 모델을 ResNet101 에서 MobilenetV2 로 교체하면 수행 속도 향상을 기대할 수 있습니다. 대신 인식 성능은 저하 될 수 있습니다.)\n",
    "\n",
    "(model zoo, 역자주:미리 학습된 모델들을 모아 놓은 공간)\n",
    "\n",
    "다음 섹션에서 우리가 어떻게 할 수 있는지 알아 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e12600",
   "metadata": {},
   "source": [
    "### 1. 미리 학습된 모델로부터 미세 조정\n",
    "COCO에 대해 미리 학습된 모델에서 시작하여 특정 클래스를 위해 미세 조정을 원한다고 가정해 봅시다. 아래와 같은 방법으로 가능합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import os\n",
    "\n",
    "os.environ['TORCH_HOME'] = 'models' #windows .cache path\n",
    "\n",
    "# COCO로 미리 학솝된 모델 읽기\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# 분류기를 새로운 것으로 교체하는데, num_classes는 사용자가 정의합니다\n",
    "num_classes = 2  # 1 클래스(사람) + 배경\n",
    "# 분류기에서 사용할 입력 특징의 차원 정보를 얻습니다\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# 미리 학습된 모델의 머리 부분을 새로운 것으로 교체합니다\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe51843",
   "metadata": {},
   "source": [
    "### 2 - 다른 백본을 추가하도록 모델을 수정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# 분류 목적으로 미리 학습된 모델을 로드하고 특징들만을 리턴하도록 합니다\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# Faster RCNN은 백본의 출력 채널 수를 알아야 합니다.\n",
    "# mobilenetV2의 경우 1280이므로 여기에 추가해야 합니다.\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# RPN(Region Proposal Network)이 5개의 서로 다른 크기와 3개의 다른 측면 비율(Aspect ratio)을 가진\n",
    "# 5 x 3개의 앵커를 공간 위치마다 생성하도록 합니다.\n",
    "# 각 특징 맵이 잠재적으로 다른 사이즈와 측면 비율을 가질 수 있기 때문에 Tuple[Tuple[int]] 타입을 가지도록 합니다.\n",
    "\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# 관심 영역의 자르기 및 재할당 후 자르기 크기를 수행하는 데 사용할 피쳐 맵을 정의합니다.\n",
    "# 만약 백본이 텐서를 리턴할때, featmap_names 는 [0] 이 될 것이라고 예상합니다.\n",
    "# 일반적으로 백본은 OrderedDict[Tensor] 타입을 리턴해야 합니다.\n",
    "# 그리고 특징맵에서 사용할 featmap_names 값을 정할 수 있습니다.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# 조각들을 Faster RCNN 모델로 합칩니다.\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653a27b",
   "metadata": {},
   "source": [
    "An Instance segmentation model for PennFudan Dataset\n",
    "In our case, we want to fine-tune from a pre-trained model, given that our dataset is very small. So we will be following approach number 1.\n",
    "\n",
    "Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de334750",
   "metadata": {},
   "source": [
    "## 모든 것을 하나로 합치기\n",
    "\n",
    "`references/detection/` 폴더 내에 검출 모델들의 학습과 평과를 쉽게 하기 위한 도움 함수들이 있습니다. \n",
    "* `references/detection/engine.py`, `references/detection/utils.py`, `references/detection/transforms.py` 를 사용 \n",
    "* `references/detection` 아래의 모든 파일과 폴더들을 사용자의 폴더로 복사한 뒤 사용합니다.\n",
    "\n",
    "데이터 증강(augmentation) / 변환을 위한 도움 함수를 작성해 봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40300e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "# Download TorchVision repo to use some files from\n",
    "# references/detection\n",
    "git clone https://github.com/pytorch/vision.git\n",
    "cd vision\n",
    "git checkout v0.3.0\n",
    "\n",
    "cp references/detection/utils.py ../\n",
    "cp references/detection/transforms.py ../\n",
    "cp references/detection/coco_eval.py ../\n",
    "cp references/detection/engine.py ../\n",
    "cp references/detection/coco_utils.py ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%shell\n",
    "\n",
    "# Download TorchVision repo to use some files from\n",
    "# references/detection\n",
    "%git clone https://github.com/pytorch/vision.git\n",
    "%cd vision\n",
    "%git checkout v0.3.0\n",
    "\n",
    "%cp references/detection/utils.py ../\n",
    "%cp references/detection/transforms.py ../\n",
    "%cp references/detection/coco_eval.py ../\n",
    "%cp references/detection/engine.py ../\n",
    "%cp references/detection/coco_utils.py ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46328c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "# chagne num_workers to zero(0) when GPU mode in windows\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b310a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb38fbe",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603870ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c404a1e",
   "metadata": {},
   "source": [
    "트레이닝이 완료되었기 때문에 하나의 test image 결과 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5002f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775418d",
   "metadata": {},
   "source": [
    "`prediction`를 출력하면 dictionary list를 확인 가능\n",
    "* 각 element가 한 이미지에 대응\n",
    "* image와 boxes, labels, masks, score를 field로 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb7b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b8988",
   "metadata": {},
   "source": [
    "Let's inspect the image and the predicted segmentation masks.\n",
    "\n",
    "For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in [C, H, W] format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fdbd7",
   "metadata": {},
   "source": [
    "### segmentation mask 확인\n",
    "* 크기: `[N, 1, H, W]`\n",
    "* N: 전체 prediction 갯수\n",
    "* value(probability map): 0 ~ 1 사이의 float 값\n",
    "* 각 class 마다 이러한 mask가 존재함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dec746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654dca40",
   "metadata": {},
   "source": [
    "# 요약\n",
    "이 튜토리얼에서는 사용자 정의 데이터셋에서 인스턴스 분할 모델을 위한 자체 학습 파이프라인을 생성하는 방법을 배웠습니다. \n",
    "이를 위해 영상과 정답 및 분할 마스크를 반환하는 torch.utils.data.Dataset 클래스를 작성했습니다. \n",
    "또한 이 새로운 데이터 셋에 대한 전송 학습(Transfer learning)을 수행하기 위해 COCO train2017에 대해 미리 학습된 Mask R-CNN 모델을 활용 했습니다.\n",
    "\n",
    "다중머신 / 다중GPU 에서의 학습을 포함하는 더 복잡한 예제를 알고 싶다면 torchvision 저장소에 있는 references/detection/train.py 를 확인해 보세요.\n",
    "\n",
    "여기 에서 이번 튜토리얼의 전체 소스코드를 다운 받으실 수 있습니다.\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/kvs1ex/d_here_are_17_ways_of_making_pytorch_training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378446e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
