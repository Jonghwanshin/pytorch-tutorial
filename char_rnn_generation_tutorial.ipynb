{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "char_rnn_generation_tutorial.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMo9oVuEbT8h"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIavDgjZbT8j"
      },
      "source": [
        "\n",
        "기초부터 시작하는 NLP:  문자-단위 RNN으로 이름 생성하기\n",
        "********************************************************************************\n",
        "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
        "  **번역**: `황성수 <https://github.com/adonisues>`_\n",
        "\n",
        "이 튜토리얼은 3개로 이뤄진 \"기초부터 시작하는 NLP\"의 2번째 튜토리얼입니다.\n",
        "`첫번째 튜토리얼 </intermediate/char_rnn_classification_tutorial>`\n",
        "에서는 이름의 언어를 분류하기 위해 RNN을 사용했습니다.\n",
        "이번에는 반대로 언어로 이름을 생성할 예정입니다.\n",
        "\n",
        "::\n",
        "\n",
        "    > python sample.py Russian RUS\n",
        "    Rovakov\n",
        "    Uantov\n",
        "    Shavakov\n",
        "\n",
        "    > python sample.py German GER\n",
        "    Gerren\n",
        "    Ereng\n",
        "    Rosher\n",
        "\n",
        "    > python sample.py Spanish SPA\n",
        "    Salla\n",
        "    Parer\n",
        "    Allan\n",
        "\n",
        "    > python sample.py Chinese CHI\n",
        "    Chan\n",
        "    Hang\n",
        "    Iun\n",
        "\n",
        "우리는 몇 개의 선형 계층으로 작은 RNN을 직접 만들고 있습니다.\n",
        "이전 튜토리얼인 이름을 읽은 후 그 언어를 예측하는 것과의 큰 차이점은\n",
        "언어를 입력하고 한 번에 한 글자를 생성하여 출력하는 것입니다.\n",
        "언어 형성(단어 또는 다른 고차원 구조로도 수행 될 수 있음)을 위해\n",
        "문자를 반복적으로 예측하는 것을 \"언어 모델\" 이라고 합니다.\n",
        "\n",
        "**추천 자료:**\n",
        "\n",
        "Pytorch를 설치했고, Python을 알고, Tensor를 이해한다고 가정합니다:\n",
        "\n",
        "-  https://pytorch.org/ 설치 안내\n",
        "-  :doc:`/beginner/deep_learning_60min_blitz` PyTorch 시작하기\n",
        "-  :doc:`/beginner/pytorch_with_examples` 넓고 깊은 통찰을 위한 자료\n",
        "-  :doc:`/beginner/former_torchies_tutorial` 이전 Lua Torch 사용자를 위한 자료\n",
        "\n",
        "RNN과 작동 방식을 아는 것 또한 유용합니다:\n",
        "\n",
        "-  `The Unreasonable Effectiveness of Recurrent Neural\n",
        "   Networks <https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__\n",
        "   실생활 예제를 보여 줍니다.\n",
        "-  `Understanding LSTM\n",
        "   Networks <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__\n",
        "   LSTM에 관한 것이지만 RNN에 관해서도 유익합니다.\n",
        "\n",
        "이전 튜토리얼도 추천합니다. :doc:`/intermediate/char_rnn_classification_tutorial`\n",
        "\n",
        "\n",
        "데이터 준비\n",
        "==================\n",
        "\n",
        ".. Note::\n",
        "   `여기 <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   에서 데이터를 다운 받고, 현재 디렉토리에 압축을 푸십시오.\n",
        "\n",
        "이 과정의 더 자세한 사항은 지난 튜토리얼을 보십시오.\n",
        "요약하면, 줄마다 이름이 적힌 텍스트 파일 ``data/names/[Language].txt`` 있습니다.\n",
        "이것을 어레이로 분리하고, Unicode를 ASCII로 변경하고,\n",
        "사전 ``{language: [names ...]}`` 을 만들어서 마무리합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC4jDABpbT8j"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters) + 1 # EOS(end of sentence) 기호 추가\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "# 유니코드 문자열을 ASCII로 변환, https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "# 파일을 읽고 줄 단위로 분리\n",
        "def readLines(filename):\n",
        "    with open(filename, encoding='utf-8') as some_file:\n",
        "        return [unicodeToAscii(line.strip()) for line in some_file]\n",
        "\n",
        "# 각 언어의 이름 목록인 category_lines 사전 생성\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "for filename in findFiles('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "if n_categories == 0:\n",
        "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
        "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
        "        'the current directory.')\n",
        "\n",
        "print('# categories:', n_categories, all_categories)\n",
        "print(unicodeToAscii(\"O'Néàl\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wMBgkDObT8k"
      },
      "source": [
        "네트워크 생성\n",
        "====================\n",
        "\n",
        "이 네트워크는 `지난 튜토리얼의 RNN <#Creating-the-Network>`__ 이\n",
        "다른 입력들과 연결되는 category tensor를 추가 인자로 가지게 확장합니다.\n",
        "category tensor는 문자 입력과 마찬가지로 one-hot 벡터입니다.\n",
        "\n",
        "역자주: 기존 입력과 category tensor를 결합하여 입력으로 사용하기 때문에\n",
        "입력의 사이즈가 n_categories 만큼 커집니다.\n",
        "\n",
        "우리는 출력을 다음 문자의 확률로 해석 합니다. 샘플링 할 때,\n",
        "가장 확률이 높은 문자가 다음 입력 문자로 사용됩니다.\n",
        "\n",
        "더 나은 동작을 위해 두 번째 선형 레이어\n",
        "``o2o`` (은닉과 출력을 결합한 후) 를 추가했습니다 .\n",
        "또한 Drop-out 계층이 있습니다. 이 계층은 주어진 확률(여기서는 0.1)로\n",
        "`무작위로 입력을 0 # <https://arxiv.org/abs/1207.0580>`__ 으로 만듭니다.\n",
        "일반적으로 입력을 흐리게 해서 과적합을 막는 데 사용됩니다.\n",
        "여기서 우리는 고의로 일부 혼돈을 추가하고 샘플링 다양성을 높이기\n",
        "위해 네트워크의 마지막에 이것을 사용합니다.\n",
        "\n",
        ".. figure:: https://i.imgur.com/jzVrf7f.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR6_DYTSbT8k"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
        "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, category, input, hidden):\n",
        "        input_combined = torch.cat((category, input, hidden), 1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(input_combined)\n",
        "        output_combined = torch.cat((hidden, output), 1)\n",
        "        output = self.o2o(output_combined)\n",
        "        output = self.dropout(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPcp-ywzbT8l"
      },
      "source": [
        "학습\n",
        "=========\n",
        "학습 준비\n",
        "----------------------\n",
        "\n",
        "제일 먼저 (category, line)의 무작위 쌍을 얻는 함수:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQVDHtskbT8l"
      },
      "source": [
        "import random\n",
        "\n",
        "# 목록에서 무작위 아이템 반환\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# 임의의 category 및 그 category에서 무작위 줄(이름) 얻기\n",
        "def randomTrainingPair():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    return category, line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_eLB_dmbT8l"
      },
      "source": [
        "각 시간 단계 마다 (즉, 학습 단어의 각 문자 마다) 네트워크의 입력은\n",
        "``(언어, 현재 문자, 은닉 상태)`` 가 되고, 출력은\n",
        "``(다음 문자, 다음 은닉 상태)`` 가 된다. 따라서 각 학습 세트 마다\n",
        "언어, 입력 문자의 세트, 출력/목표 문자의 세트가 필요하다.\n",
        "\n",
        "각 시간 단계마다 현재 문자에서 다음 문자를 예측하기 때문에,\n",
        "문자 쌍은 한 줄(하나의 이름)에서 연속된 문자 그룹입니다. - 예를 들어 ``\"ABCD<EOS>\"`` 는\n",
        "(\"A\", \"B\"), (\"B\", \"C\"), (\"C\", \"D\"), (\"D\", \"EOS\") 로 생성합니다.\n",
        "\n",
        ".. figure:: https://i.imgur.com/JH58tXY.png\n",
        "   :alt:\n",
        "\n",
        "Category(언어) Tensor는 ``<1 x n_categories>`` 크기의 `One-hot\n",
        "Tensor <https://en.wikipedia.org/wiki/One-hot>`__ 입니다.\n",
        "학습시에 모든 시간 단계에서 네트워크에 이것을 전달합니다.\n",
        "- 이것은 설계 선택사항으로, 초기 은닉 상태 또는\n",
        "또 다른 전략의 부분으로 포함될 수 있습니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJa3H8WVbT8l"
      },
      "source": [
        "# Category를 위한 One-hot 벡터\n",
        "def categoryTensor(category):\n",
        "    li = all_categories.index(category)\n",
        "    tensor = torch.zeros(1, n_categories)\n",
        "    tensor[0][li] = 1\n",
        "    return tensor\n",
        "\n",
        "# 입력을 위한 처음부터 마지막 문자(EOS 제외)까지의  One-hot 행렬\n",
        "def inputTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li in range(len(line)):\n",
        "        letter = line[li]\n",
        "        tensor[li][0][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# 목표를 위한 두번째 문자 부터 마지막(EOS) 까지의 LongTensor\n",
        "def targetTensor(line):\n",
        "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
        "    letter_indexes.append(n_letters - 1) # EOS\n",
        "    return torch.LongTensor(letter_indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhW1AKgjbT8m"
      },
      "source": [
        "학습 동안 편의를 위해 무작위로 (category[언어], line[이름])을 가져오고\n",
        "그것을 필요한 형태 (category[언어], input[현재 문자], target[다음 문자]) Tensor로 바꾸는\n",
        "``randomTrainingExample`` 함수를 만들 예정입니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQOvgp-abT8m"
      },
      "source": [
        "# 임의의 Category에서 Category, Input, Target Tensor를 만듭니다.\n",
        "def randomTrainingExample():\n",
        "    category, line = randomTrainingPair()\n",
        "    category_tensor = categoryTensor(category)\n",
        "    input_line_tensor = inputTensor(line)\n",
        "    target_line_tensor = targetTensor(line)\n",
        "    return category_tensor, input_line_tensor, target_line_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOeOU-5RbT8m"
      },
      "source": [
        "네트워크 학습\n",
        "--------------------\n",
        "\n",
        "마지막 출력만 사용하는 분류와 달리, 모든 단계에서 예측을 수행하므로\n",
        "모든 단계에서 손실을 계산합니다.\n",
        "\n",
        "Autograd의 마법이 각 단계의 손실들을 간단하게 합하고 마지막에\n",
        "역전파를 호출하게 해줍니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MaPOojSbT8m"
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
        "    target_line_tensor.unsqueeze_(-1)\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
        "        l = criterion(output, target_line_tensor[i])\n",
        "        loss += l\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
        "\n",
        "    return output, loss.item() / input_line_tensor.size(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7QgcDI9bT8m"
      },
      "source": [
        "학습에 걸리는 시간을 추적하기 위해 사람이 읽을 수 있는 문자열을\n",
        "반환하는``timeSince (timestamp)`` 함수를 추가합니다:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt1_Yk4fbT8n"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH_qSvc5bT8n"
      },
      "source": [
        "학습은 일상적인 일입니다. - 몇번 train() 을 호출하고, 몇 분 정도\n",
        "기다렸다가 ``print_every`` 마다 현재 시간과 손실을 출력하고,\n",
        "나중에 도식화를 위해  ``plot_every`` 마다 ``all_losses`` 에\n",
        "평균 손실을 저장합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e9lRcTCbT8n"
      },
      "source": [
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0 # plot_every 마다 초기화\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    output, loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every)\n",
        "        total_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jomVjiPEbT8n"
      },
      "source": [
        "손실 도식화\n",
        "-------------------\n",
        "\n",
        "all\\_losses를 이용한 손실의 도식화는\n",
        "네트워크의 학습 상태를 보여줍니다:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQn-1MuHbT8n"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNDlrkMPbT8n"
      },
      "source": [
        "네트워크 샘플링\n",
        "====================\n",
        "\n",
        "샘플링을 위해서, 네트워크에 하나의 글자를 주고 다음 문자를 물어보고\n",
        "이것을 다음 문자로 전달하는 것을 EOS 토큰까지 반복합니다.\n",
        "\n",
        "-  입력 카테고리(언어), 시작 문자, 비어 있는 은닉 상태를 위한 Tensor를 생성하십시오\n",
        "-  시작 문자로 ``output_name`` 문자열을 생성하십시오\n",
        "-  최대 출력 길이까지,\n",
        "\n",
        "   -  현재 문자를 네트워크에 전달하십시오.\n",
        "   -  가장 높은 출력에서 다음 문자와 다음 은닉 상태를 얻으십시오\n",
        "   -  만일 문자가 EOS면, 여기서 멈추십시오\n",
        "   -  만일 일반적인 문자라면, ``output_name`` 에 추가하고 계속하십시오\n",
        "\n",
        "-  마지막 이름을 반환하십시오\n",
        "\n",
        ".. Note::\n",
        "   시작 문자를 주는 것 외에 \"문자열 시작\" 토큰을 학습에\n",
        "   포함되게 하고 네트워크가 자체적으로 시작 문자를 선택하게 하는\n",
        "   다른 방법도 있습니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_cvBN3ubT8n"
      },
      "source": [
        "max_length = 20\n",
        "\n",
        "# 카테고리와 시작 문자로 부터 샘플링 하기\n",
        "def sample(category, start_letter='A'):\n",
        "    with torch.no_grad():  # 샘플링에서 히스토리를 추적할 필요 없음\n",
        "        category_tensor = categoryTensor(category)\n",
        "        input = inputTensor(start_letter)\n",
        "        hidden = rnn.initHidden()\n",
        "\n",
        "        output_name = start_letter\n",
        "\n",
        "        for i in range(max_length):\n",
        "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
        "            topv, topi = output.topk(1)\n",
        "            topi = topi[0][0]\n",
        "            if topi == n_letters - 1:\n",
        "                break\n",
        "            else:\n",
        "                letter = all_letters[topi]\n",
        "                output_name += letter\n",
        "            input = inputTensor(letter)\n",
        "\n",
        "        return output_name\n",
        "\n",
        "# 하나의 카테고리와 여러 시작 문자들로 여러 개의 샘플 얻기\n",
        "def samples(category, start_letters='ABC'):\n",
        "    for start_letter in start_letters:\n",
        "        print(sample(category, start_letter))\n",
        "\n",
        "samples('Russian', 'RUS')\n",
        "\n",
        "samples('German', 'GER')\n",
        "\n",
        "samples('Spanish', 'SPA')\n",
        "\n",
        "samples('Chinese', 'CHI')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqKsuQObbT8o"
      },
      "source": [
        "Exercises\n",
        "=========\n",
        "\n",
        "-  Try with a different dataset of category -> line, for example:\n",
        "\n",
        "   -  Fictional series -> Character name\n",
        "   -  Part of speech -> Word\n",
        "   -  Country -> City\n",
        "\n",
        "-  Use a \"start of sentence\" token so that sampling can be done without\n",
        "   choosing a start letter\n",
        "-  Get better results with a bigger and/or better shaped network\n",
        "\n",
        "   -  Try the nn.LSTM and nn.GRU layers\n",
        "   -  상위 수준 네트워크로 여러 개의 이런 RNN을  결합해 보십시오\n",
        "\n",
        "\n"
      ]
    }
  ]
}